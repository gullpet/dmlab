{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gullpet/dmlab/blob/master/Lab03-03/LabTopic3c_using_several_classifiers_nocode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-5fUKN2Tbm3"
      },
      "source": [
        "# Using several classifiers and tuning parameters - Parameters grid\n",
        "[From official `scikit-learn` documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html)\n",
        "\n",
        "Adapted by Claudio Sartori\n",
        "\n",
        "Example of usage of the ***model selection*** features of `scikit-learn` and comparison of several classification methods.\n",
        "1. import a sample dataset \n",
        "1. split the dataset into two parts: train and test\n",
        "    - the *train* part will be used for training and validation (i.e. for *development*)\n",
        "    - the *test* part will be used for test (i.e. for *evaluation*)\n",
        "    - the fraction of test data will be _ts_ (a value of your choice between 0.2 and 0.5)\n",
        "1. the function `GridSearchCV` iterates a cross validation experiment to train and test a model with different combinations of paramater values\n",
        "    - for each parameter we set a list of values to test, the function will generate all the combinations\n",
        "    - we choose a *score function* which will be used for the optimization\n",
        "        - e.g. `accuracy_score`, `precision_score`, `cohen_kappa_score`, `f1_score`, see this [page](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for reference\n",
        "    - the output is a dictionary containing \n",
        "        - the set of parameters which maximize the score \n",
        "        - the test scores\n",
        "1. prepare the parameters for the grid\n",
        "    - it is a list of dictionaries\n",
        "1. set the parameters by cross validation and the *score functions* to choose from\n",
        "1. Loop on scores and, for each score, loop on the model labels (see details below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQkjd9r7Tbm6",
        "outputId": "afa3c7d6-5dad-4fe1-a6f5-972285d44907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
            "@author: scikit-learn.org and Claudio Sartori\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
        "@author: scikit-learn.org and Claudio Sartori\n",
        "\"\"\"\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # uncomment this line to suppress warnings\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "\n",
        "print(__doc__) # print information included in the triple quotes at the beginning\n",
        "\n",
        "# Loading a standard dataset\n",
        "#dataset = datasets.load_digits()\n",
        "#dataset = datasets.fetch_olivetti_faces()\n",
        "#dataset = datasets.fetch_covtype()\n",
        "dataset = datasets.load_iris()\n",
        "#dataset = datasets.load_wine()\n",
        "#dataset = datasets.load_breast_cancer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YK6ip91Tbm8"
      },
      "source": [
        "### Prepare the environment\n",
        "The `dataset` module contains, among others, a few sample datasets.\n",
        "\n",
        "See this [page](http://scikit-learn.org/stable/datasets/index.html) for reference\n",
        "\n",
        "Prepare the data and the target in X and y. Set `ts`. Set the random state to 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v0_0sts-Tbm9"
      },
      "outputs": [],
      "source": [
        "X = dataset.data\n",
        "y = dataset.target\n",
        "ts = 0.3\n",
        "random_state = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpj2jWIkTbm-"
      },
      "source": [
        "Split the dataset into the train and test parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuEIfYwcTbm-",
        "outputId": "a985fcbe-0b01-46a1-edbe-a59ff8d0ba95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 112 samples\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
        "print(\"Training on\", len(y_train), \"samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VChxsoXFTbm_"
      },
      "source": [
        "The code below is intended to ease the remainder of the exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4aZh7t7MTbnA"
      },
      "outputs": [],
      "source": [
        "model_lbls = [\n",
        "              'dt', \n",
        "              'nb', \n",
        "              'lp', \n",
        "              'svc', \n",
        "             'knn',\n",
        "             'adb',\n",
        "             'rf',\n",
        "            ]\n",
        "\n",
        "# Set the parameters by cross-validation\n",
        "tuned_param_dt = [{'max_depth': [*range(1,20)]}]\n",
        "tuned_param_nb = [{'var_smoothing': [10, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-07, 1e-8, 1e-9, 1e-10]}]\n",
        "tuned_param_lp = [{'early_stopping': [True]}]\n",
        "tuned_param_svc = [{'kernel': ['rbf'], \n",
        "                    'gamma': [1e-3, 1e-4],\n",
        "                    'C': [1, 10, 100, 1000],\n",
        "                    },\n",
        "                    {'kernel': ['linear'],\n",
        "                     'C': [1, 10, 100, 1000],                     \n",
        "                    },\n",
        "                   ]\n",
        "tuned_param_knn =[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
        "tuned_param_adb = [{'n_estimators':[20,30,40,50],\n",
        "                   'learning_rate':[0.5,0.75,1,1.25,1.5]}]\n",
        "tuned_param_rf = [{'max_depth': [*range(5,15)],\n",
        "                   'n_estimators':[*range(10,100,10)]}]\n",
        "\n",
        "models = {\n",
        "    'dt': {'name': 'Decision Tree       ',\n",
        "           'estimator': DecisionTreeClassifier(), \n",
        "           'param': tuned_param_dt,\n",
        "          },\n",
        "    'nb': {'name': 'Gaussian Naive Bayes',\n",
        "           'estimator': GaussianNB(),\n",
        "           'param': tuned_param_nb\n",
        "          },\n",
        "    'lp': {'name': 'Linear Perceptron   ',\n",
        "           'estimator': Perceptron(),\n",
        "           'param': tuned_param_lp,\n",
        "          },\n",
        "    'svc':{'name': 'Support Vector      ',\n",
        "           'estimator': SVC(), \n",
        "           'param': tuned_param_svc\n",
        "          },\n",
        "    'knn':{'name': 'K Nearest Neighbor ',\n",
        "           'estimator': KNeighborsClassifier(),\n",
        "           'param': tuned_param_knn\n",
        "       },\n",
        "       'adb':{'name': 'AdaBoost           ',\n",
        "           'estimator': AdaBoostClassifier(),\n",
        "           'param': tuned_param_adb\n",
        "          },\n",
        "    'rf': {'name': 'Random forest       ',\n",
        "           'estimator': RandomForestClassifier(),\n",
        "           'param': tuned_param_rf\n",
        "          }\n",
        "\n",
        "}\n",
        "\n",
        "scores = ['precision', 'recall']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSzjM2xSTbnB"
      },
      "source": [
        "### The function below groups all the outputs\n",
        "Write a function which has as parameter the fitted model and uses the components of the fitted model to inspect the results of the search with the parameters grid.\n",
        "\n",
        "The components are:<br>\n",
        "`model.best_params_`<br>\n",
        "`model.cv_results_['mean_test_score']`<br>`\n",
        "model.cv_results_['std_test_score']`<br>\n",
        "`model.cv_results_['params']`\n",
        "\n",
        "The classification report is generated by the function imported above from sklearn.metrics, which takes as argument the true and the predicted test labels.\n",
        "\n",
        "The +/- in the results is obtained doubling the `std_test_score`\n",
        "\n",
        "The function will be used to print the results for each set of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hQ9sFXjNTbnD"
      },
      "outputs": [],
      "source": [
        "def print_results(model):\n",
        "    print(\"Best parameters set found on train set:\")\n",
        "    print()\n",
        "    # if best is linear there is no gamma parameter\n",
        "    print(model.best_params_)\n",
        "    print()\n",
        "    print(\"Grid scores on train set:\")\n",
        "    print()\n",
        "    means = model.cv_results_['mean_test_score']\n",
        "    stds = model.cv_results_['std_test_score']\n",
        "    params = model.cv_results_['params']\n",
        "    for mean, std, params_tuple in zip(means, stds, params):\n",
        "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "              % (mean, std * 2, params_tuple))\n",
        "    print()\n",
        "    print(\"Detailed classification report for the best parameter set:\")\n",
        "    print()\n",
        "    print(\"The model is trained on the full train set.\")\n",
        "    print(\"The scores are computed on the full test set.\")\n",
        "    print()\n",
        "    y_true, y_pred = y_test, model.predict(X_test)\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jeekc1ITbnD"
      },
      "source": [
        "### Loop on scores and, for each score, loop on the model labels\n",
        "- iterate varying the score function\n",
        "    1. iterate varying the classification model among Decision Tree, Naive Bayes, Linear Perceptron, Support Vector, AdaBoost, Random Forest and KNN\n",
        "        - activate the *grid search*\n",
        "            1. the resulting model will be the best one according to the current score function\n",
        "        - print the best parameter set and the results for each set of parameters using the above defined function\n",
        "        - print the classification report\n",
        "        - store the `.best score_` in a dictionary for a final report\n",
        "    1. print the final report for the current *score funtion*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QXfwutITbnE",
        "outputId": "ffcc62c7-dd2e-42dd-a650-0643a9f9ae3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying model Decision Tree       \n",
            "Best parameters set found on train set:\n",
            "\n",
            "{'max_depth': 6}\n",
            "\n",
            "Grid scores on train set:\n",
            "\n",
            "0.696 (+/-0.033) for {'max_depth': 1}\n",
            "0.902 (+/-0.130) for {'max_depth': 2}\n",
            "0.928 (+/-0.110) for {'max_depth': 3}\n",
            "0.937 (+/-0.124) for {'max_depth': 4}\n",
            "0.946 (+/-0.089) for {'max_depth': 5}\n",
            "0.964 (+/-0.036) for {'max_depth': 6}\n",
            "0.964 (+/-0.036) for {'max_depth': 7}\n",
            "0.937 (+/-0.124) for {'max_depth': 8}\n",
            "0.937 (+/-0.124) for {'max_depth': 9}\n",
            "0.946 (+/-0.089) for {'max_depth': 10}\n",
            "0.964 (+/-0.036) for {'max_depth': 11}\n",
            "0.937 (+/-0.124) for {'max_depth': 12}\n",
            "0.937 (+/-0.124) for {'max_depth': 13}\n",
            "0.964 (+/-0.036) for {'max_depth': 14}\n",
            "0.937 (+/-0.124) for {'max_depth': 15}\n",
            "0.937 (+/-0.124) for {'max_depth': 16}\n",
            "0.946 (+/-0.089) for {'max_depth': 17}\n",
            "0.955 (+/-0.058) for {'max_depth': 18}\n",
            "0.946 (+/-0.089) for {'max_depth': 19}\n",
            "\n",
            "Detailed classification report for the best parameter set:\n",
            "\n",
            "The model is trained on the full train set.\n",
            "The scores are computed on the full test set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      0.94      0.97        16\n",
            "           2       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.97        38\n",
            "   macro avg       0.97      0.98      0.97        38\n",
            "weighted avg       0.98      0.97      0.97        38\n",
            "\n",
            "\n",
            "Trying model Gaussian Naive Bayes\n",
            "Best parameters set found on train set:\n",
            "\n",
            "{'var_smoothing': 0.01}\n",
            "\n",
            "Grid scores on train set:\n",
            "\n",
            "0.696 (+/-0.033) for {'var_smoothing': 10}\n",
            "0.884 (+/-0.090) for {'var_smoothing': 1}\n",
            "0.928 (+/-0.093) for {'var_smoothing': 0.1}\n",
            "0.946 (+/-0.068) for {'var_smoothing': 0.01}\n",
            "0.937 (+/-0.093) for {'var_smoothing': 0.001}\n",
            "0.937 (+/-0.093) for {'var_smoothing': 0.0001}\n",
            "0.937 (+/-0.093) for {'var_smoothing': 1e-05}\n",
            "0.937 (+/-0.093) for {'var_smoothing': 1e-06}\n",
            "0.937 (+/-0.093) for {'var_smoothing': 1e-07}\n",
            "0.937 (+/-0.093) for {'var_smoothing': 1e-08}\n",
            "0.937 (+/-0.093) for {'var_smoothing': 1e-09}\n",
            "0.937 (+/-0.093) for {'var_smoothing': 1e-10}\n",
            "\n",
            "Detailed classification report for the best parameter set:\n",
            "\n",
            "The model is trained on the full train set.\n",
            "The scores are computed on the full test set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      0.94      0.97        16\n",
            "           2       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.97        38\n",
            "   macro avg       0.97      0.98      0.97        38\n",
            "weighted avg       0.98      0.97      0.97        38\n",
            "\n",
            "\n",
            "Trying model Linear Perceptron   \n",
            "Best parameters set found on train set:\n",
            "\n",
            "{'early_stopping': True}\n",
            "\n",
            "Grid scores on train set:\n",
            "\n",
            "0.714 (+/-0.076) for {'early_stopping': True}\n",
            "\n",
            "Detailed classification report for the best parameter set:\n",
            "\n",
            "The model is trained on the full train set.\n",
            "The scores are computed on the full test set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      1.00      0.81        13\n",
            "           1       0.00      0.00      0.00        16\n",
            "           2       0.47      1.00      0.64         9\n",
            "\n",
            "    accuracy                           0.58        38\n",
            "   macro avg       0.39      0.67      0.49        38\n",
            "weighted avg       0.35      0.58      0.43        38\n",
            "\n",
            "\n",
            "Trying model Support Vector      \n",
            "Best parameters set found on train set:\n",
            "\n",
            "{'C': 1, 'kernel': 'linear'}\n",
            "\n",
            "Grid scores on train set:\n",
            "\n",
            "0.696 (+/-0.033) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.366 (+/-0.045) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.884 (+/-0.090) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.696 (+/-0.033) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.973 (+/-0.044) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.884 (+/-0.090) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.973 (+/-0.044) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.973 (+/-0.044) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.982 (+/-0.044) for {'C': 1, 'kernel': 'linear'}\n",
            "0.946 (+/-0.105) for {'C': 10, 'kernel': 'linear'}\n",
            "0.928 (+/-0.146) for {'C': 100, 'kernel': 'linear'}\n",
            "0.946 (+/-0.134) for {'C': 1000, 'kernel': 'linear'}\n",
            "\n",
            "Detailed classification report for the best parameter set:\n",
            "\n",
            "The model is trained on the full train set.\n",
            "The scores are computed on the full test set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      1.00      1.00        16\n",
            "           2       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           1.00        38\n",
            "   macro avg       1.00      1.00      1.00        38\n",
            "weighted avg       1.00      1.00      1.00        38\n",
            "\n",
            "\n",
            "Trying model K Nearest Neighbor \n",
            "Best parameters set found on train set:\n",
            "\n",
            "{'n_neighbors': 9}\n",
            "\n",
            "Grid scores on train set:\n",
            "\n",
            "0.938 (+/-0.092) for {'n_neighbors': 1}\n",
            "0.947 (+/-0.105) for {'n_neighbors': 2}\n",
            "0.946 (+/-0.037) for {'n_neighbors': 3}\n",
            "0.956 (+/-0.080) for {'n_neighbors': 4}\n",
            "0.937 (+/-0.074) for {'n_neighbors': 5}\n",
            "0.956 (+/-0.055) for {'n_neighbors': 6}\n",
            "0.955 (+/-0.055) for {'n_neighbors': 7}\n",
            "0.964 (+/-0.066) for {'n_neighbors': 8}\n",
            "0.973 (+/-0.044) for {'n_neighbors': 9}\n",
            "0.973 (+/-0.044) for {'n_neighbors': 10}\n",
            "\n",
            "Detailed classification report for the best parameter set:\n",
            "\n",
            "The model is trained on the full train set.\n",
            "The scores are computed on the full test set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      0.94      0.97        16\n",
            "           2       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.97        38\n",
            "   macro avg       0.97      0.98      0.97        38\n",
            "weighted avg       0.98      0.97      0.97        38\n",
            "\n",
            "\n",
            "Trying model AdaBoost           \n",
            "Best parameters set found on train set:\n",
            "\n",
            "{'learning_rate': 1, 'n_estimators': 20}\n",
            "\n",
            "Grid scores on train set:\n",
            "\n",
            "0.902 (+/-0.130) for {'learning_rate': 0.5, 'n_estimators': 20}\n",
            "0.902 (+/-0.130) for {'learning_rate': 0.5, 'n_estimators': 30}\n",
            "0.902 (+/-0.130) for {'learning_rate': 0.5, 'n_estimators': 40}\n",
            "0.902 (+/-0.130) for {'learning_rate': 0.5, 'n_estimators': 50}\n",
            "0.902 (+/-0.130) for {'learning_rate': 0.75, 'n_estimators': 20}\n",
            "0.902 (+/-0.130) for {'learning_rate': 0.75, 'n_estimators': 30}\n",
            "0.902 (+/-0.130) for {'learning_rate': 0.75, 'n_estimators': 40}\n",
            "0.902 (+/-0.130) for {'learning_rate': 0.75, 'n_estimators': 50}\n",
            "0.964 (+/-0.036) for {'learning_rate': 1, 'n_estimators': 20}\n",
            "0.920 (+/-0.129) for {'learning_rate': 1, 'n_estimators': 30}\n",
            "0.937 (+/-0.124) for {'learning_rate': 1, 'n_estimators': 40}\n",
            "0.920 (+/-0.129) for {'learning_rate': 1, 'n_estimators': 50}\n",
            "0.964 (+/-0.036) for {'learning_rate': 1.25, 'n_estimators': 20}\n",
            "0.946 (+/-0.089) for {'learning_rate': 1.25, 'n_estimators': 30}\n",
            "0.964 (+/-0.036) for {'learning_rate': 1.25, 'n_estimators': 40}\n",
            "0.946 (+/-0.089) for {'learning_rate': 1.25, 'n_estimators': 50}\n",
            "0.964 (+/-0.036) for {'learning_rate': 1.5, 'n_estimators': 20}\n",
            "0.964 (+/-0.036) for {'learning_rate': 1.5, 'n_estimators': 30}\n",
            "0.964 (+/-0.036) for {'learning_rate': 1.5, 'n_estimators': 40}\n",
            "0.955 (+/-0.002) for {'learning_rate': 1.5, 'n_estimators': 50}\n",
            "\n",
            "Detailed classification report for the best parameter set:\n",
            "\n",
            "The model is trained on the full train set.\n",
            "The scores are computed on the full test set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      0.94      0.97        16\n",
            "           2       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.97        38\n",
            "   macro avg       0.97      0.98      0.97        38\n",
            "weighted avg       0.98      0.97      0.97        38\n",
            "\n",
            "\n",
            "Trying model Random forest       \n",
            "Best parameters set found on train set:\n",
            "\n",
            "{'max_depth': 6, 'n_estimators': 10}\n",
            "\n",
            "Grid scores on train set:\n",
            "\n",
            "0.937 (+/-0.124) for {'max_depth': 5, 'n_estimators': 10}\n",
            "0.937 (+/-0.124) for {'max_depth': 5, 'n_estimators': 20}\n",
            "0.937 (+/-0.124) for {'max_depth': 5, 'n_estimators': 30}\n",
            "0.937 (+/-0.124) for {'max_depth': 5, 'n_estimators': 40}\n",
            "0.937 (+/-0.124) for {'max_depth': 5, 'n_estimators': 50}\n",
            "0.937 (+/-0.124) for {'max_depth': 5, 'n_estimators': 60}\n",
            "0.937 (+/-0.124) for {'max_depth': 5, 'n_estimators': 70}\n",
            "0.928 (+/-0.124) for {'max_depth': 5, 'n_estimators': 80}\n",
            "0.928 (+/-0.124) for {'max_depth': 5, 'n_estimators': 90}\n",
            "0.955 (+/-0.058) for {'max_depth': 6, 'n_estimators': 10}\n",
            "0.937 (+/-0.124) for {'max_depth': 6, 'n_estimators': 20}\n",
            "0.937 (+/-0.124) for {'max_depth': 6, 'n_estimators': 30}\n",
            "0.937 (+/-0.124) for {'max_depth': 6, 'n_estimators': 40}\n",
            "0.937 (+/-0.124) for {'max_depth': 6, 'n_estimators': 50}\n",
            "0.937 (+/-0.124) for {'max_depth': 6, 'n_estimators': 60}\n",
            "0.937 (+/-0.124) for {'max_depth': 6, 'n_estimators': 70}\n",
            "0.919 (+/-0.134) for {'max_depth': 6, 'n_estimators': 80}\n",
            "0.928 (+/-0.124) for {'max_depth': 6, 'n_estimators': 90}\n",
            "0.946 (+/-0.089) for {'max_depth': 7, 'n_estimators': 10}\n",
            "0.928 (+/-0.124) for {'max_depth': 7, 'n_estimators': 20}\n",
            "0.937 (+/-0.124) for {'max_depth': 7, 'n_estimators': 30}\n",
            "0.937 (+/-0.124) for {'max_depth': 7, 'n_estimators': 40}\n",
            "0.937 (+/-0.124) for {'max_depth': 7, 'n_estimators': 50}\n",
            "0.928 (+/-0.124) for {'max_depth': 7, 'n_estimators': 60}\n",
            "0.928 (+/-0.124) for {'max_depth': 7, 'n_estimators': 70}\n",
            "0.937 (+/-0.124) for {'max_depth': 7, 'n_estimators': 80}\n",
            "0.937 (+/-0.124) for {'max_depth': 7, 'n_estimators': 90}\n",
            "0.919 (+/-0.134) for {'max_depth': 8, 'n_estimators': 10}\n",
            "0.946 (+/-0.089) for {'max_depth': 8, 'n_estimators': 20}\n",
            "0.937 (+/-0.124) for {'max_depth': 8, 'n_estimators': 30}\n",
            "0.937 (+/-0.124) for {'max_depth': 8, 'n_estimators': 40}\n",
            "0.937 (+/-0.124) for {'max_depth': 8, 'n_estimators': 50}\n",
            "0.937 (+/-0.124) for {'max_depth': 8, 'n_estimators': 60}\n",
            "0.937 (+/-0.124) for {'max_depth': 8, 'n_estimators': 70}\n",
            "0.937 (+/-0.124) for {'max_depth': 8, 'n_estimators': 80}\n",
            "0.937 (+/-0.124) for {'max_depth': 8, 'n_estimators': 90}\n",
            "0.946 (+/-0.089) for {'max_depth': 9, 'n_estimators': 10}\n",
            "0.937 (+/-0.124) for {'max_depth': 9, 'n_estimators': 20}\n",
            "0.937 (+/-0.124) for {'max_depth': 9, 'n_estimators': 30}\n",
            "0.928 (+/-0.124) for {'max_depth': 9, 'n_estimators': 40}\n",
            "0.928 (+/-0.124) for {'max_depth': 9, 'n_estimators': 50}\n",
            "0.937 (+/-0.124) for {'max_depth': 9, 'n_estimators': 60}\n",
            "0.955 (+/-0.058) for {'max_depth': 9, 'n_estimators': 70}\n",
            "0.928 (+/-0.124) for {'max_depth': 9, 'n_estimators': 80}\n",
            "0.928 (+/-0.124) for {'max_depth': 9, 'n_estimators': 90}\n",
            "0.937 (+/-0.124) for {'max_depth': 10, 'n_estimators': 10}\n",
            "0.937 (+/-0.124) for {'max_depth': 10, 'n_estimators': 20}\n",
            "0.937 (+/-0.124) for {'max_depth': 10, 'n_estimators': 30}\n",
            "0.937 (+/-0.124) for {'max_depth': 10, 'n_estimators': 40}\n",
            "0.937 (+/-0.124) for {'max_depth': 10, 'n_estimators': 50}\n",
            "0.937 (+/-0.124) for {'max_depth': 10, 'n_estimators': 60}\n",
            "0.928 (+/-0.124) for {'max_depth': 10, 'n_estimators': 70}\n",
            "0.928 (+/-0.124) for {'max_depth': 10, 'n_estimators': 80}\n",
            "0.937 (+/-0.124) for {'max_depth': 10, 'n_estimators': 90}\n",
            "0.928 (+/-0.124) for {'max_depth': 11, 'n_estimators': 10}\n",
            "0.937 (+/-0.124) for {'max_depth': 11, 'n_estimators': 20}\n",
            "0.937 (+/-0.093) for {'max_depth': 11, 'n_estimators': 30}\n",
            "0.919 (+/-0.134) for {'max_depth': 11, 'n_estimators': 40}\n",
            "0.937 (+/-0.124) for {'max_depth': 11, 'n_estimators': 50}\n",
            "0.919 (+/-0.134) for {'max_depth': 11, 'n_estimators': 60}\n",
            "0.928 (+/-0.124) for {'max_depth': 11, 'n_estimators': 70}\n",
            "0.937 (+/-0.124) for {'max_depth': 11, 'n_estimators': 80}\n",
            "0.937 (+/-0.124) for {'max_depth': 11, 'n_estimators': 90}\n",
            "0.928 (+/-0.124) for {'max_depth': 12, 'n_estimators': 10}\n",
            "0.937 (+/-0.124) for {'max_depth': 12, 'n_estimators': 20}\n",
            "0.937 (+/-0.124) for {'max_depth': 12, 'n_estimators': 30}\n",
            "0.937 (+/-0.124) for {'max_depth': 12, 'n_estimators': 40}\n",
            "0.937 (+/-0.124) for {'max_depth': 12, 'n_estimators': 50}\n",
            "0.919 (+/-0.134) for {'max_depth': 12, 'n_estimators': 60}\n",
            "0.937 (+/-0.124) for {'max_depth': 12, 'n_estimators': 70}\n",
            "0.928 (+/-0.124) for {'max_depth': 12, 'n_estimators': 80}\n",
            "0.937 (+/-0.124) for {'max_depth': 12, 'n_estimators': 90}\n",
            "0.928 (+/-0.123) for {'max_depth': 13, 'n_estimators': 10}\n",
            "0.937 (+/-0.124) for {'max_depth': 13, 'n_estimators': 20}\n",
            "0.937 (+/-0.124) for {'max_depth': 13, 'n_estimators': 30}\n",
            "0.937 (+/-0.124) for {'max_depth': 13, 'n_estimators': 40}\n",
            "0.919 (+/-0.134) for {'max_depth': 13, 'n_estimators': 50}\n",
            "0.928 (+/-0.124) for {'max_depth': 13, 'n_estimators': 60}\n",
            "0.928 (+/-0.124) for {'max_depth': 13, 'n_estimators': 70}\n",
            "0.937 (+/-0.124) for {'max_depth': 13, 'n_estimators': 80}\n",
            "0.937 (+/-0.124) for {'max_depth': 13, 'n_estimators': 90}\n",
            "0.946 (+/-0.089) for {'max_depth': 14, 'n_estimators': 10}\n",
            "0.937 (+/-0.093) for {'max_depth': 14, 'n_estimators': 20}\n",
            "0.937 (+/-0.124) for {'max_depth': 14, 'n_estimators': 30}\n",
            "0.928 (+/-0.124) for {'max_depth': 14, 'n_estimators': 40}\n",
            "0.937 (+/-0.124) for {'max_depth': 14, 'n_estimators': 50}\n",
            "0.928 (+/-0.124) for {'max_depth': 14, 'n_estimators': 60}\n",
            "0.937 (+/-0.124) for {'max_depth': 14, 'n_estimators': 70}\n",
            "0.937 (+/-0.124) for {'max_depth': 14, 'n_estimators': 80}\n",
            "0.937 (+/-0.124) for {'max_depth': 14, 'n_estimators': 90}\n",
            "\n",
            "Detailed classification report for the best parameter set:\n",
            "\n",
            "The model is trained on the full train set.\n",
            "The scores are computed on the full test set.\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      0.94      0.97        16\n",
            "           2       0.90      1.00      0.95         9\n",
            "\n",
            "    accuracy                           0.97        38\n",
            "   macro avg       0.97      0.98      0.97        38\n",
            "weighted avg       0.98      0.97      0.97        38\n",
            "\n",
            "\n",
            "FINAL REPORT\n",
            "-------------- Best scores --------------\n",
            "Decision Tree       \t: 0.9644268774703558\n",
            "Gaussian Naive Bayes\t: 0.9462450592885375\n",
            "Linear Perceptron   \t: 0.7138339920948615\n",
            "Support Vector      \t: 0.982213438735178\n",
            "K Nearest Neighbor \t: 0.9731225296442687\n",
            "AdaBoost           \t: 0.9640316205533598\n",
            "Random forest       \t: 0.9553359683794467\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "score_dict = {}\n",
        "\n",
        "for label in model_lbls:\n",
        "  print(\"Trying model\", models[label]['name'])\n",
        "  estimator = GridSearchCV(models[label]['estimator'], models[label]['param'], scoring='accuracy')\n",
        "  estimator.fit(X_train, y_train)\n",
        "  print_results(estimator)\n",
        "  # strip of excess whitespaces\n",
        "  label = label.strip()\n",
        "  score_dict[models[label]['name']] = np.max(estimator.cv_results_['mean_test_score'])\n",
        "\n",
        "print(\"FINAL REPORT\")\n",
        "print(\"-------------- Best scores --------------\")\n",
        "for label in score_dict.keys():\n",
        "  if len(label) < 17:\n",
        "    print(label + \"\\t\\t:\", score_dict[label])\n",
        "  else:\n",
        "    print(label + \"\\t:\", score_dict[label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pPK57a4OTbnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "outputId": "1faace29-630c-4837-a470-9283973140c1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAANOCAYAAABgFv8rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdfZBld13n8c/XBAiKhGUzUkACEzS4GwUijBHxKQpiAkp0YYEUqHHRqGtQEdGsWBGDpSA+LQJiUAxQSBJ8YEfImkVgwEWQTICEPFRwCCAJlAwPsoXyFPjtH+cMuel0T/fM9Mx8e/r1qpqac889fe6vz7n39H3fc/t2jTECAABAH19xuAcAAADAbQk1AACAZoQaAABAM0INAACgGaEGAADQzNGH64aPO+64sXXr1sN18wAAAIfVlVde+bExxpblrjtsobZ169bs3LnzcN08AADAYVVVH1zpOm99BAAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM2sGmpV9dKq+mhVXbPC9VVVz6+qXVV1dVU9eP2HCQAAsHms5YzaRUlO38v1ZyQ5af53TpI/OvBhAQAAbF6rhtoY4y1JPrGXRc5M8vIxeXuSu1XVPddrgAAAAJvNevyO2r2TfGjh8k3zvNupqnOqamdV7dy9e/c63DQAAMCR55B+mMgY48IxxrYxxrYtW7YcypsGAADYMNYj1G5OcsLC5ePneQAAAOyH9Qi17Ul+ZP70x4cm+dQY4yPrsF4AAIBN6ejVFqiqVyU5LclxVXVTkl9LcockGWO8OMllSR6VZFeSf0/yYwdrsAAAAJvBqqE2xjhrletHkp9ZtxEBAABscof0w0QAAABYnVADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKCZVf/gNQAAvW0973WHewhHvA8859GHewhsMs6oAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGR/PDwAAh5E/r3DwbcQ/r+CMGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmvHx/ACsCx8vffBtxI+XBmD/CDWOKJ4oHhqeLAIAHFze+ggAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzRx9uAfQzdbzXne4h7ApfOA5jz7cQ6Ahj7+Dz2MPADYGZ9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaGZNoVZVp1fVDVW1q6rOW+b6+1TVm6rqXVV1dVU9av2HCgAAsDmsGmpVdVSSFyY5I8nJSc6qqpOXLParSS4dY3xTkicmedF6DxQAAGCzWMsZtVOT7Bpj3DjG+HySi5OcuWSZkeSu8/SxST68fkMEAADYXNYSavdO8qGFyzfN8xY9K8mTq+qmJJcleepyK6qqc6pqZ1Xt3L17934MFwAA4Mi3Xh8mclaSi8YYxyd5VJJXVNXt1j3GuHCMsW2MsW3Lli3rdNMAAABHlrWE2s1JTli4fPw8b9FTklyaJGOMtyU5Jslx6zFAAACAzWYtoXZFkpOq6sSqumOmDwvZvmSZf07y8CSpqv+cKdS8txEAAGA/rBpqY4xbkpyb5PIk12f6dMdrq+qCqnrMvNjTk/xEVV2V5FVJzh5jjIM1aAAAgCPZ0WtZaIxxWaYPCVmcd/7C9HVJvm19hwYAALA5rdeHiQAAALBOhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANHP04R4AAHD4bT3vdYd7CEe8Dzzn0Yd7CMAG4owaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJpZU6hV1elVdUNV7aqq81ZY5vFVdV1VXVtVf76+wwQAANg8jl5tgao6KskLk3xvkpuSXFFV28cY1y0sc1KS/5Hk28YYn6yqrzlYAwYAADjSreWM2qlJdo0xbhxjfD7JxUnOXLLMTyR54Rjjk0kyxvjo+g4TAABg81hLqN07yYcWLt80z1t0/yT3r6q3VtXbq+r05VZUVedU1c6q2rl79+79GzEAAMARbr0+TOToJCclOS3JWUleUlV3W7rQGOPCMca2Mca2LVu2rNNNAwAAHFnWEmo3Jzlh4fLx87xFNyXZPsb4whjj/UnemyncAAAA2EdrCbUrkpxUVSdW1R2TPDHJ9iXLvCbT2bRU1XGZ3gp54zqOEwAAYNNYNdTGGLckOTfJ5UmuT3LpGOPaqrqgqh4zL3Z5ko9X1XVJ3pTkGWOMjx+sQQMAABzJVv14/iQZY1yW5LIl885fmB5JfmH+BwAAwAFYrw8TAQAAYJ0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0MyaQq2qTq+qG6pqV1Wdt5flHltVo6q2rd8QAQAANpdVQ62qjkrywiRnJDk5yVlVdfIyy311kp9L8o/rPUgAAIDNZC1n1E5NsmuMceMY4/NJLk5y5jLLPTvJc5N8dh3HBwAAsOmsJdTuneRDC5dvmud9WVU9OMkJY4zXrePYAAAANqUD/jCRqvqKJL+X5OlrWPacqtpZVTt37959oDcNAABwRFpLqN2c5ISFy8fP8/b46iTfmGRHVX0gyUOTbF/uA0XGGBeOMbaNMbZt2bJl/0cNAABwBFtLqF2R5KSqOrGq7pjkiUm277lyjPGpMcZxY4ytY4ytSd6e5DFjjJ0HZcQAAABHuFVDbYxxS5Jzk1ye5Pokl44xrq2qC6rqMQd7gAAAAJvN0WtZaIxxWZLLlsw7f4VlTzvwYQEAAGxeB/xhIgAAAKwvoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJpZU6hV1elVdUNV7aqq85a5/heq6rqqurqq3lBV913/oQIAAGwOq4ZaVR2V5IVJzkhycpKzqurkJYu9K8m2McYDk/xFkt9e74ECAABsFms5o3Zqkl1jjBvHGJ9PcnGSMxcXGGO8aYzx7/PFtyc5fn2HCQAAsHmsJdTuneRDC5dvmuet5ClJ/veBDAoAAGAzO3o9V1ZVT06yLcl3rXD9OUnOSZL73Oc+63nTAAAAR4y1nFG7OckJC5ePn+fdRlU9IskzkzxmjPG55VY0xrhwjLFtjLFty5Yt+zNeAACAI95aQu2KJCdV1YlVdcckT0yyfXGBqvqmJH+cKdI+uv7DBAAA2DxWDbUxxi1Jzk1yeZLrk1w6xri2qi6oqsfMiz0vyV2SvLqq3l1V21dYHQAAAKtY0++ojTEuS3LZknnnL0w/Yp3HBQAAsGmt6Q9eAwAAcOgINQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaEaoAQAANCPUAAAAmhFqAAAAzQg1AACAZoQaAABAM0INAACgGaEGAADQjFADAABoRqgBAAA0I9QAAACaEWoAAADNCDUAAIBmhBoAAEAzQg0AAKAZoQYAANCMUAMAAGhGqAEAADQj1AAAAJoRagAAAM0INQAAgGaEGgAAQDNCDQAAoBmhBgAA0IxQAwAAaGZNoVZVp1fVDVW1q6rOW+b6O1XVJfP1/1hVW9d7oAAAAJvFqqFWVUcleWGSM5KcnOSsqjp5yWJPSfLJMcbXJfn9JM9d74ECAABsFms5o3Zqkl1jjBvHGJ9PcnGSM5csc2aSl83Tf5Hk4VVV6zdMAACAzaPGGHtfoOpxSU4fY/z4fPmHk3zLGOPchWWumZe5ab78vnmZjy1Z1zlJzpkvfn2SG9brG9nkjkvysVWXoiv7b2Oz/zY2+2/jsu82NvtvY7P/1s99xxhblrvi6EM5ijHGhUkuPJS3uRlU1c4xxrbDPQ72j/23sdl/G5v9t3HZdxub/bex2X+Hxlre+nhzkhMWLh8/z1t2mao6OsmxST6+HgMEAADYbNYSalckOamqTqyqOyZ5YpLtS5bZnuRH5+nHJXnjWO09lQAAACxr1bc+jjFuqapzk1ye5KgkLx1jXFtVFyTZOcbYnuRPk7yiqnYl+USmmOPQ8XbSjc3+29jsv43N/tu47LuNzf7b2Oy/Q2DVDxMBAADg0FrTH7wGAADg0BFqAAAAzWyaUKuqe1TVn1fVjVV1ZVW9rap+6BDc7raqev46rWtHVe1csu4dq3zNvarqL9bhtrdW1Weq6t1VdVVV/UNVff2BrvdQqaofrKpRVf9pL8vsqKq9ftTsvMwN83a4fv7bgOs5zrOr6l7ruc6NoKq+OG/Ta6rqb6rqbuu03rOr6gXrsa4l6/2Oqrp2HvOd13v98238ysFYbycL+/3a+bjy9Krar59LVXVBVT1iL9f/VFX9yP6PNqmqB8zjfXdVfaKq3j9P/92BrLebqvr0MvMOePvtxzj2HG+vqqq3Ho6fOVV1SlU96lDf7sFQVc+cH2tXz/fbbzmMY/n5qvrKZeb/WlX91pJ5p1TV9fu4/rtV1X8/0HEebIuPtap6VFW9t6ruu2SZs6vqS1X1wIV511TV1kM30i/f7mlV9bAVrtuvcVbVn1TVyassc9H8d52XG89r1zb6jWlThFpVVZLXJHnLGON+Y4yHZPrAk+MP9m2PMXaOMX52HVf5NVV1xj7c/ofHGLe7c++n940xThljPCjJy5JspCeSZyX5v/P/B+pJY4xTknxbkufOn4a6Xs5OsulCLcln5vvWN2b6QKKfOdwDWsWTkvzWPObPrLbw/GdL9tVGenztrz37/RuSfG+SM5L82v6saIxx/hhjxWAaY7x4jPHy/RznnnW8Zx7vKZk+7fgZ8+UvB+J+7uv21mP77U1NlntO8qSFnznPO8B17Y9TkiwbahtpX1fVtyb5/iQPHmM8MMkjknzoMI3lqCQ/n+R2oZbkVUmesGTeE+f5++JuSfYp1A7n/qyqhyd5fpIzxhgfXGaRm5I88yDc7r5+z6clWTbUZvs8zjHGj48xrtvHcayL+b7Y2qYItSTfk+TzY4wX75kxxvjgGOMPky+fLfr7qnrn/O9h8/zblHpVvaCqzp6nn1NV182vTP3OPO+/zq8eXFVVb1m6jqo6taYzee+qhTNS86sQf1VVf1tV/1RVv72X7+V5WeZBsJfvYWtVXTNPv72qvmHha3bUdFbuq6rqpVX1jnlsZ65hm941ySdXue2XV9UPLtzeK6vqzKo6qqqeV1VXzNvvJ+fr71lVb6lbz6x8xxrGsaqqukuSb0/ylCx8ImlV3bmqLq7pzNhfJ7nzwnV/VFU751cff32FVd8lyb8l+eL8NWdV1XvmsT93YV23mz9vg4vmee+pqqfNrxZtS/LKOohnajaAtyW5d7J/j5mq+rGaXpV8R6aY3jN/a1W9cb7PvaGq7jPPv2je32+v6Yz7afPj4fqqumjp4Krqx5M8Psmz5/t0zffnPfvyCfNyp82Pi+1JrtuX+31VPSfJned5rzxI27mVMcZHk5yT5Nx5my67vZKkqn553tZXzdvqNq+41vLH52dV1S/O06fM+/vqqvrrqvoP8/wdVfXc+Vj43rUeg+av+4Oa3vHwc1X1kKp6c03v3ri8qu45L/e18332yvm+seIZ/m6WbL9lt9Ne7uN3mR9z75z325nz/K01nTF7eZJrctu/2brUW5J83fx1z1i4jV9faV0r3E+W3Qfz/efFNR3331tV31/Ti3AXJHnC/Fh8wrwdXlFVb830add7O648v6bj1o21zNmAQ+yeST42xvhckowxPjbG+HCSVNUHquq4efrL79RZ+F7fVtNx9ifm+afNx6zXzdv8xTWHca38c/DTVfW7VXVVpucw90rypqp60+IgxxjvTfLJuu3ZvscnedVe9t095sfxVfO/hyV5TpKvnffb82qy6nF6nbf5mlTVdyZ5SZLvH2O8b4XFXpvkG2qZs8pV9ch5H72zql5d03OeVNX580MBMxEAAA1WSURBVOPkmqq6sKpqnr/W49XP1q3H0YtrOjP2U0meNm/X5Y6P+zPOHTW/m6mqnjI//t5RVS+p274j5jtXeDzd9QDui9+6wvbuY4xxxP9L8rNJfn8v139lkmPm6ZMy/dmBZHrl4LULy70g0xmP/5jkhtz6qZl3m/9/T5J7L5n35XVkipuj5+lHJPnLefrsJDdm+kPhxyT5YJITlhnnjkxP5N+Y5Lvn6R2rfA9bk1wzTz8tya/P0/dMcsM8/ZtJnrxn3Enem+Srltz21iSfSfLuJO9L8pEk91nltr8ryWvm6WOTvD/Tn4Q4J8mvzvPvlGRnkhOTPD3JM+f5RyX56nXa/09K8qfz9D8kecg8/QuZ/txEkjwwyS1Jts2X774wjh1JHriwD25IcvW8PX5ynn+vJP+cZMv8Pb4xyQ/uZf5Dkrx+YYx3W1j/tsP9mDkMj9FPL2zvVyc5fX8eM/P9es/2vmOStyZ5wfw1f5PkR+fp/7Zw37woycVJKsmZSf5fkgdkeiHryiSnLDPei5I8bp5+bJLXz2O/x3z798z02P+3JCfOy+3T/X7PNjmS/y33PSb513k7rrS9zpgfx185X7fnsXpRpr/judLx+VlJfnGevjrJd83TFyT5g3l6R5LfnacfleTv9jL2xfvAjiQvmqfvMI9vy3z5Cbn1OPOGJCfN09+S6W+OHvb9sMb9srj9lt1Oe9lnRye56zz/uCS75sfb1iRfSvLQFcaxI7cek5+R5JIkj8z0seCV6TH62iTfuXRde7mfLLsP5v35t/M6T8p0ZuCYTMeaFyzZDlcmufN8eW/HlVfP6zs5ya7DvE/vkunn93uTvGjP/X++7gNJjpunF59XPCvJVZlexDwu0xm4e2U6tn02yf0yHbNen+mxt+zPu3ldI8njl7vNZcb6i5mfsyV5aG59TrHSvrskyc/P00dl+rmwNfNzn3n+mo7Th2G/fCHTu0geuJdlzs70/PNHkrxsnnfN/D0el+lFjK+a5/9ykvMX7/Pz9CuS/MDC42otx6sPJ7nTPH274+g6jnPHfL+713y/uPs8rr/PrT+/L8oyj6esw32x+78Nc9p+PVXVCzOdYfn8GOObM90hXlBVp2Q6O3L/VVbxqUx3jD+t6WzZnrNub01yUVVdmuSvlvm6Y5O8rKpOynRHucPCdW8YY3xqHt91Se6bld+W8BtJfjXTHX2PtXwPlyb5P5neWvT4JHt+d+2RSR5T86ulmX443SfJ0veEv29Mb/nJ/GrUhUlOX+m2xxhvrqoXVdWWTAfJvxzT3+V7ZJIHLrwicmymH4xXJHlpVd0h0w+7d6/w/e+rs5L8z3n64vnylZl+uD9/HuvVVXX1wtc8vqbfPzs608H85ExP7pLprTg75+/rH6rqbzO9PWbHGGP3vH1eOa9/rDD/2UnuV1V/mOR1mfbLZnbnqnp3pjNp12c62Cb7/pg5Lrfd3pfk1sfCtyb5L/P0K5Isnrn+mzHGqKr3JPmXMcZ75q+/NtMPmb3dF789yavGGF9M8i9V9eYk35wp+N4xxnj/vNyhvt9vdCttr0ck+bMxxr8nyRjjE0u+bqXjc5Kkqo7N9KTjzfOsl2V6ArDHnmP3lZn2/VpdMv//9Um+Mcnr5xewj0rykfnV44clefU8P5liZqNabjuttM9uSvKb85mDL2V6nN9jXuaDY4y37+V2XllVn8n0BO6pSX5uvp13zdffZb6Nf16yrtvdT9awDy4dY3wpyT9V1Y1JVjrjuX3c+pbnvR1XXjOv77qqukcOozHGp6vqIUm+I9MLvZdU1XljjItW+dL/NX+vn6np7NepmV5MeccY48YkqapXZToOfiHL/7x7TabnBn+5xuFekuln69Mzv+1xlX33PZniIPNx+FM1nyVfsNbj9KH2hUyh9JRM9+29+fMkz6yqExfmPTTT85O3ztvljpnelZIk311Vv5TpxfS7J7k20wsLySrHq/m6qzM9/l6TaR+u1b6Oc49Tk7x5zzG9ql6d2z6XXenxdDDvi4fdZgm1azOFQpJkjPEzNZ3m3/PBHE9L8i9JHpSp1j87z78lt3176DHz199SVacmeXimcj83yfeMMX6qptP1j05y5XxQXPTsJG8aY/zQfAp5x8J1n1uY/mL2sm/GGG+sqt/IdMffY6XvYfHrbq6qj9f0i55PyHQKO5lemXzsGOOGlW5zGduT/NkabvvlSZ6c6WD7Ywu399QxxuVLVzr/IH90puD9vXGAvxNRVXfPdBB/QFWNTAehUVXP2MvXnJjpFb1vHmN8sqa3vx2zdLkxxu6qememV/Y+t/T6vZnX+6Ak35dpPzw+06uxm9Vnxhin1PTL5Zdn+h2152edHjNrsGddX1qy3i8d4Hr/bWH6kN3vN6qqul+mffnRrLC9qur79raOlY7P+zCMPft/X+9Te/Z1Jbl2jHGbt9RU1V2T/OueF7uOAMttp5X22dmZXtl+yBjjC1X1gdx6TF18jCznSWOMxQ/Rqky/H/rHS25j6xrW9RXZ+z4Yq1zeY7Xb2WPxWFIrLnWIzJGyI8mO+UWpH810pmLxuc7Sn3UrbZO1bqs9Pjvf/lrG+aGqen+md+U8NlMMr7bvDsRa9+fB8KVMP//fUFW/Msb4zZUWnI9tv5vbvkhfmd6dc5vfv6+qYzKdOd02b89n5bb7dq/Hq9mjM8XND2QKrwes5Rval3Huo5UeTwftvtjBZvkdtTcmOaaqfnph3uIvsR6b5CNzqf9wpifzyfR2qpOr6k41fQrdw5Mv/87TsWOMyzJFyoPm+V87xvjHMcb5SXbn9u+3PzbJzfP02Qf4Pf1Gkl9aw/ew1CXz1x07xthzhujyJE+dfwCmqr5pDbf/7ZneArnabV+U6ZeGM279ZdHLk/z0fAYhVXX/mn5P7r6Zzma8JMmfJHnwGsaxmscl+f/tnUtonFUUx3+nURBNKVJbF1JUBC2pCwVxI2rdCYoPJJQYtGhd6EJdKKLgojQL0QriA61FNGJLQVuCWLCIUkVra2Nsk6ZqN0a04qOK4gOkLo6Lc4aZfP3mEWfSjMn/ByHMzTd3vrmP8917z/+cvObu57r7ee6+gpBgXkm44W/Ne7iYkD9CyO3+Ik7lziYkNCeQm4pLiXbYD1xtZmdZBKcOAB/UK8+DgkXuvoPwjla+6x/A4g587/8lefp9H/CARZDzTOfMJ0R7L83x1V/zt4+pxigOErKKTvAhEcPSk17Wq4h+LzLTcf9P5dqFQLbdJkLq4tRpL8LbekfOv8phTG09pfa5Qnphf7VqfMVtxFztFEeAZRbJGzCzU81slbv/DkyZWX+WWx7WzCfq9dkS4KfcpF1DeL/b+Yw7rRrfco6ZLS+57oRx0kIf9JvZIjO7gJBSHaG5TZ4tu9JRzOwiC2VChUuINQ6Et7JysHwL07nRzE4zs6WEzGw0yy83s/Mt4oHWEMm66j0Hy2jWrtuAp4Cv3P1ok757D7gny3ssvObF+lu10yedfO5dBwya2bomlw8T3uJl+XofcIWZVeI3zzCzC6luyn7OuVIvRrLUXmW/rnD33cSGawnhvW51jdLqfdYySoyfM/P5XxyL9Wh3LHY1C2Kjlg/9m4hOm7JIMvAq1d3+88Bai8DCleRJg7t/S8gFJ/N3RWqxGNhpIZX7iIh1AthoGbhIGO/xwq08ATxmZgdo05uZi5BjNUWl36GE7cRD5fWasiFCUjZhIfUaqvPeSmDuOBHXdlezz3b3Hwkp2ys19bxEBO1+lm31ItEeq4HxbJ81VOWK7TAAjBTKdmT5C0CvRdrfDYSEB3cfJ/r6S8KFv6fw/q0WMr0xYNjdx9z9e+BhYDfR72Pu/ma9ckL6837WswV4JOseBjbZAk4m4u4HCMnFADOcM9ne6wlJxR6my3fvJRZuE8TivJnMpFVG8n7HiUOhh9z9h5LrZjruNxNzcj4nE6kkTDkMvEtIgCvJe0rby913ER79T3P+PFios559rmUtYa8niAXrhk59IXc/TiyKHk+beJBqlrRBYF2WHyZiIruR083saM1PWRuWUW+MbwUus/Di3E7Y1v+Eu79D2OW9Wd92ShaODcZJoz74hljgvQ3c7e5/E7a7L8dpMRshzJ5d6TS9hIz887zXPsJWQsy5py2SSxQ9DRNEG+wDhjwTkBCL6ucIGzsFjDR43pWxGdhlhWQiNbwBrGJ6tsd6fXc/IfM7RDyX+9z9F0JmN2lmG2ndTs8JKfe7FnjUzG5ocN1xQm2yPF8fIw4xt2W/7gVWuvtvRIKSSeJwY7RBfWX2qgfYkm16AHgm63wLuNnqJxOZ0X0W3vMdsbbcTzy/vyak7M1odyx2NZVgayFmhTzNPESkBG5lwgkhhBAnFQuJ+053b/v/js4XLORyf7r7k4Xy1URCievn4r7E/MXMejOe8hRic/2yuxcP2xcUC8KjJuYGi38++wXwrDZpQgghhBCiAevTAz5JeMdmksRkXiKPmhBCCCGEEEJ0GfKoCSGEEEIIIUSXoY2aEEIIIYQQQnQZ2qgJIYQQQgghRJehjZoQQgghhBBCdBnaqAkhhBBCCCFEl/Evmz2keGLe+FAAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.bar(score_dict.keys(), score_dict.values())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len('Linear Perceptron'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0OqVucdIVwl",
        "outputId": "7a571861-79c1-4bd9-dfbe-d41b434752c3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLg28sKBNn5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('res')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "7650257707f3238d5df88771c66da47b78c5077cb779498608c81dcf9deec5b5"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}